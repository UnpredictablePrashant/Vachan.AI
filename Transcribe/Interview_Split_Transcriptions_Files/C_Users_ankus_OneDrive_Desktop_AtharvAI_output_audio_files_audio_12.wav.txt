Transcription for C:\Users\ankus\OneDrive\Desktop\AtharvAI\output_audio_files\audio_12.wav:
okay so can you tell me the current project that you're working on so basically my work is related to like data scientist they create so it's basically email office is not like not detailed pipelines creation so mostly what I do is like data scientists create their own machine learning models and I deploy them I create pipelines in airflow to deploy those models into the virtual environment like staging and production okay so that's the yeah and what programming languages can you just tell me like what are the services and what was the flow like how are you social services so for sure like mostly for detail pipeline creation I have is 88 as you did a factory so I was creating Dynamic detail pipelines there and I mean including all those variable things and everything and I was utilizing like keyboard to store all this like credential parts and everything and then I should be so I still integrated and create some dynamic programming inside so what we had in the database notebook that in the customer service internet getting generated in the database notebook and then we used to published out in the month or sometime today what is the difference between data Lake and italic is basically same so but in the Delta Lake we have some added advantage of like we have Community catalog where we can add some quality checks and also like permissions to the different layer of data like we have silver layer brunch near so we can use that you look at look at that advantage of that later okay so what is the exact use of as you mentioned what is exactly like why is it needed in the first place so suppose there's any companies that like listening and not all of them are all of them need to be getting access to all layers of data so supposedly start working in the project and they don't need to get access to the old lady they just need to get access to the clean part like a cold and let's say data Engineers they need to access the role of data where all the transformation losses will be applied all the cleaning parts will be done so that's why we need to separate all these things so like Enterprise level software we need to give access properly so people can work like there will be more like data governance so have you worked on data profiling any tools or anything not exactly OK Google in the first place let's see some sample data you need it basically creates a generic script OK Google set the values of correctly properly populated for those five columns it shouldn't be like name column has some integer values so this mapping this kind of thing data profiling does so you mentioned about data governance as well could you just take me like what you how you have implemented data governance in any of your projects using in the catalog or any other tool just an idea of what you understand by data governance yeah so yeah in our project only we have to do like the auditor is to come from different sources sometimes it's coming from streaming from there we used to collect all the data into a little late or maybe less location and from there to like transform the data clean the data and put it into it has been separated you different locations in defense independent locations or something create different files are maybe sometimes in a different School database or maybe later so it's not it was not fixed for every scenario there are different requirements from the client site so based on that give me an example of One Republic Services different type of permissions so no one I mean not everyone is still able to access all this players and then he needs to go to the gold and go there will assess was given to the high level employers means that manager or maybe senior send it so yeah so that's how it was divided into the different layers and that's how it is to apply and all this information and all these things were I mean credentials were stored in keywords and keyboards access was given to from Azure this is like single thing okay so understood so what I just want to understand from you is like can you just tell me one or two or three maybe key features of the unity catalog like you said that's perfectly fine anything apart from that because just one one liners or some words would like some key features if you would say from what you have worked on what do you understand that Unity catalog exactly of course yeah one is due to governance and another is like the Storage level means that some data need not to be stored in like there are different levels of like sometimes we don't only to store the historical data for last 30 days or maybe some last one year so in row level I mean in the Bronx level live within didn't need to apply this kind of like like all data I mean sometimes it is to store it in a cold storage need it not to be accessed a very frequently but in the morning that access control as well yeah access control and like storage or sometimes what I can say like not which data was not able to access frequently was not can we also see that it offers the flow from the destination exactly how that is coming so basically it helps also in debugging right if there's any failure in between also in the pipeline you can use that for purpose as well that's good so okay just go through some of the most features to offer so some of them more okay all right so okay one more thing I want to know is two things I want to know is how much data you are dealing with and what kind of data is it what kind of files you are dealing with and what kind of data is in historical or incremental Delta load whatever so just explain that so data first of all data was no it was not limited even for now so it's not like fixed so mostly because from 1GB to 5G depends more than that and yeah and also can you please repeat history is it incremental what kind of data is it a number three is what kind of are you getting from the source and what is the file that it is being transformed into yeah sure so yeah mostly files I'm getting generated like CSV files and wish to convert the parking for me basically used to do that and yeah and what kind of data data history project we are not using such kind of things but in my old projects so you have been using Azure as well right now is this new thing in the market for sure have you had the chance to work on it not exactly mostly it was like in part of okay all right so can you tell me that what are the advantages of parking over other file formats what can first of all what kind of file format is so it is easier for when it's a problem and also it has like it is a compression technique so that's why the bike parks are more the size are very low temperature for some kind of aggregation so only we can take all this particular columns for the education do you know what predicate full stop what what exactly like what are the things why do you worked on Spotify it is temporary stored in particular station in the memories so it's very much more faster to access we can go on the data and also all this operations we are performing in the data frame it will be happening inside the memory and as like Spark so all these things will be like in the collection data shuffling stage it will be much more optimized if you're using data from okay and what about so there's a basic difference between data set and data frame can you call something you play we're not doing some pudding in the notebook so whenever I run the code after that we get home so that is run time safe today okay if you have your scholarly will see that during compilation only it will show you that it has this is a difference like compiled type safety gives you the error between during compile time only run time keeps doing that time so that's the basic difference between data and data frame as well dataframes do not give you compile time safety you will see it after you run but they said if you use if you convert into a data set you will get compiled and safety so this is a very big difference of other advantages so it depends on which one you want to use okay okay so you mentioned about lazy evaluation of can you explain that information so whenever there so those things can be like happening in white transformation so that time data needs to be shuffled in the like from 1:30 a.m. so when a white transmission is happening before that when the transmission is happening in a particular partition then all these things are like taking into the dead or in a graph all these things are not being compared so when you're collecting the data or maybe you have some white transformation and so it will wait for the college operation or maybe even so that's why it's called you okay so basically you what what is the conclusion like why is why is it so you mentioned about that do you know about lineage graph as well what is lineage graph something related to rdds yeah I mean I have to cancel OK Google what happens if what is happening in every step so basically if you added three is lost you get it from the lineage of this is what the things that were applied so that's the importance of lineage graph okay okay so you can Transformations so can you name some of the Transformations which you have used in your project and how do you use it like what was the use case and why was it needed yeah I mean there was a first level of use cases as if there was a lot of requirements to come from the customer service how can we know avoid that I mean we cannot avoid joining obviously we need to join two tables but how can we avoid I mean listen to shuffling before even joining the date of friends because I don't have a small world in them it will be a lot of traffic and I want to optimize that in this scenario I want to optimize that so what will be your approach to solve this problem first of all I need to understand why we were doing a little like after joining will be doing some operations into that pool table light that is the like that is the client has said that it's a must we will have to join so we can do like all the operations which will be happening and we can start all the parts that will be easier if you're joining the tables it will be already sorted the both parts and also so different different so maybe we are doing some applications or maybe some so we can do that part in the before joining also after doing that after something and then we can do that so that will listen to write so have you walked on police and repetition I have used can you explain like what is the difference and what it does reposition means like partitions can be increased or even decreased but call is only increase the volume decrease the volume so we are like we are like marching to partitions as a cold or something maybe we can join I mean join based on a particular criteria not join but maybe get a credit report from is created what happens after that I mean what happens after that doesn't what is the exact thing that happens after you submit this background there is like a plan gets created that how it will be executed in the background then it goes to the AQ means adaptive credit execution and it selects the most efficient method executed and that is in spark I think there's a version for it for a QE there's a version lyrics of some spiritual and above like a cube is enabled what I'm saying is that it respective of the version of spark so okay what are partitions first tell me what are partitions the condition is the partition of the data or maybe this town and yeah so that's basically partition yeah that is based on the data I'm talking in terms of spark has this partitions you can so the partitions as in like let's say 5 and those are divided into 11444 right so that is how it is created because the partitions will be created and then the data will Shuffle among the partitions know so that is the basic so that's what I'm saying so when you submit the spark of the partitions are created and then what you are saying is based on the data the data I mean across the data we will do the partitions that this will go into this partition and this will go into depart this partition so my scenario is when this partitions are created when you submit the spark shop number one how are the partitions created I mean what what is the criteria for this to be decided that how many practitioners will be created number one and number two is let's say you have to decrease the number of partitions okay so we have to options polish or republican so which one will be efficient for you are or which one would you choose to call Patricia okay so far how it is getting partition means first of all we can give the criteria the based on how it can partition if you don't give the credit then it will create like hash partition based on that will be partition the data I can see partition has maximum of 118 MB So based on how much data is there based on that little divide the partition so that I mean we can calculate the number of partitions based on that total number of data and amount of data and yeah cool is and reposition I think I'll be for reducing the data I think Coalition will be more efficient but I don't exactly remember the reason why you can't answer so have you used police station because there was a particular scenario that we have I had to calculate cumulative count for some data so some repeating data was coming and we still have a calculator that how many times did it's getting repeated before that I used to call is the data and yeah so where does school is happening because it happened in the driver many of the executor it happens to executed correct so and where does the partitions department is the most effective way let's say when your day is divided okay you don't understand 90% of the data and other partitions are empty more or less because 10% of that is just distributed so in this case there will be what there will be particular scenario what is the name of that one partition that is something more data that job will take a long time building more time as well because it has to process all that data and others are sitting empty so we have to eventually distribute the data so how can we do that we can either use School list or repetition but in this case if you see that one partition is creating more it contains more data don't you think that a full shuffling is needed because full shuffling what will it do it again full stop I mean evenly across the partitions so this is the kind of scenario when you can use repetition okay and cold is as you said like if you want to merge something is a better option yeah so yeah I mean to be exactly okay yeah so yeah more or less we are covered with spark their just go through like the you know basic concepts of it you know what happens in the back and when you run a spark job how are this partitions created in this partitions also have a relation with the number of CPU course your assigning when you run the spark job you assign some parameters as well write like number of course it will contain and how much memory you are giving so these are also I mean creating the number of particles there's a relationship with you see if you are able to access that OK Google what is the runtime for Publix what is a round tables configurations for using that what cluster you will use and there's some parameters also to be set so what is what is the date that you have used that is already assigned before projects so yeah okay so we will just cover python first okay so can you tell me the difference between deep copy and shallow coffee so run something into that objects found in the original and deep copy Concepts a new compound object and then recursively in search copies into the object on the door so what is the difference like what is the copy doing what is the exact difference in Shallow copy as you understand from the name itself right now references not the exact options OK Google okay okay have you walked on Pandora can you tell me the difference between a series and a data frame series means like as a list of talking there's a concept in Panda Express restaurant crazy run I'll write can you share your screen itself resume hey okay 97 you cannot read it two word streamers that is up to you Google can you tell me that I can check if the starting that if it begins with the same letter or not so I need to check the first percent increase will you tell me to Water Street so I need a look anyway so how can I check the first letter so I need to check for each other okay 0 0 1 okay just work on the syntax after the interview okay so you're splitting it and then taking the first index of the first word and first chocolate is in small letters so will it match when is an upper one is in small so will it return to convertible 1000 afterwards okay so just walk on the send text message to lower and then yeah 00 first letter first first word first letter second letter or second word first letter that you need to check okay just walk on the sentence that's the feedback okay okay I will quickly cover sequel as we are a bit out of time so okay I'll give you a scenario over here itself so let's say you know case statement right let's say you have kissed when to equal to bars and one equal to 9 then then three scenarios 3 van conditions what will be the output of each of them I forgot the difference so here it is not comparing the volume eight okay okay one last question is what is the difference between ranked and rank and row number and explain just with an example let's give you a data set like let's say name H salary what is the partition here Partition by name or is whatever considering partitions and ordered by everything lecture that is done just tell me that how will it assign the ranks over 4064 celery the salary it will give the angles one one okay okay so thank you for the internet 2008833 you just need to I think a little bit more yeah so right when the rank what it does is when it has died over here right it is 6060 is a time so 1:15 but it keeps the next track this will be three and this will be four it keeps number two because it is to two of them then set this problem it gives same number but it does not skip anything with the interview

