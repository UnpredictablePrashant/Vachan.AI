Transcription for C:\Users\ankus\OneDrive\Desktop\AtharvAI\output_audio_files\audio_13.wav:
welcome Raj for this interview on Azure data engineering road so can you please tell me about yourself I have completed my engineering in your 2019 from Mumbai University since then I'm having total 5 years of experience out of which my initial 2.5 years I have spent as a ETL consultant in which I was basically working with the databases and data warehousing and my recent 2.5 years of experience is in as a engineer in which I am using Azure data Factory database and have a good command over right spot and I have a background noise I have been into Sequel and this is Christmas so can you tell me about your project so my project is basically of like Insurance to me we are following basically Delta lake house architecture and here we are having like a million architecture which is on a Delta lake house so we are having a basically 3 years from Silver and Gold what we are doing is we are getting our data from multiple sources out of which one is from on Prem sequel TV which is nothing we are getting our three basic oil customer data Branch data and agent data from on premises so like that from other data we are getting in the form of directly from Upstream system in form of other because in insurance company we have to take a good understanding about weather as well so weather in formation we get from rest API once this information is better we just pulling this data with the help of ADF and loading it into our area is location and once this data is needed location then we perform different cleansing optimal activities and according to the business logic we are finally loading the data into our goal here and then from that our listing of power bi people and it can take away from there so what is the team size we are having we are seven people and others and how many pipelines you have in your project so yeah so in our project we are having 13355 minutes okay so far so it's like it's recently started project because 7 people and we have just like 30 35 lines so oh no I mean I have I have made 35 planes and in total in total there are more than 150 200 pipelines are there a big project okay so what is the most challenging situation which you face you have created a couple of pipelines so you might have encounter some of the problems so what do you feel that this is one of the most challenging situation you have been came across challenging situation yeah yeah recently we got this thing you know requirement from the client side that a certain pipeline should work only on you know only the business days like only when there are no holidays for the company so in schedule window in the schedule what we are having their we can just set like only to Monday to Friday we can set but there was this problem which we were facing if Suppose there is 15 August 15th holiday how to do that so far that was I took the initiative to myself what I did was I prepared directly or Excel file and which we put all the holidays which were there for our company from the company's calendar and just before running that pipeline we were having a basically we were having a look up activity that look up activity used to go into that file and check whether it is a holiday then we were putting a slope and if only if that particular date is not there then pipeline will run and in schedule schedule we are just putting it Monday to Friday so in this manner we have like made it possible yeah okay got it great so that's great so you said that you have and on premises Odessa system from which you are pulling the data so can you tell me what was exact that dear Source on what kind of a database it is and how exactly you pull the data from it is a SQL database Azure SQL database and we are pulling the data with the help of a copy of the music is it an on-premise is a cloud one oh oh my bad okay I was just thinking about another it was on premises okay now how you how you pull the data from Oracle on Prem to the cloud with the help of copy activity just only copy activities location in this pipeline we will use a a run time so actually there is also resolve integration run time which will not work in this case because we are pulling the data from on-prem so the data Google the data I hold it hold it you say normally Auto resolution's TV and it will provide the competition power over there from on premium and on premise and then only we can pull the data so generally why I mean the question means him why you need this helpful to indication run time why normal Auto resignation and time will not going to work when you feeling from this on-prem okay no worries no worries yourself it is coming into the landing container okay then yeah yeah yeah once the data is there in landing container with then we are again using the copy activity and we are pushing this data into the bronzer okay so you use a copy activity to push the return to the bronze layer so where is this is what kind of it is your hair is only okay bronze lyrics only now you copy the data from one Landing Zone to the translate okay then copied now then yeah once our data is there another Promise lyrics we add like flag to it so that when it when it is going to next year then we can identify this is this flag is just added to distinguish which data we have pushed further and which is a remaining so this is how the flag thing works so if my understanding is good you said that you first pull the data from your on Prem to the ADLs and that is in terms of a file or table yes yes is it yes it is a table so how you keep the table in adela's save it as a table it is it is getting saved underneath and in the form of it is a pocket file in adela's environment okay so it is a file in 88 ads in okay then what is the Brown's latest yes it is a it is a pocket file and only okay it is a pacifier but it is a table or it is a fine it is a fine just that we can access the file in the form of table because of this data lake house architecture we are using we are implementing Delta Lake up on it it is like a frame what do you say will go step by step OK Google then again you are using copy activity to move the data from your lending to the bronze layer right that is also a copy activity now where the Delta Lake comes in here you ask right so what is the format so that is why you keep it into the you copy it as a using copy activity you copying it as a file just wright Park a fighter so we are saving this file in right and this package is not operational in which we can what do DML operation and acid properties are there and we this this file will behave like a table and we can do all the commands like we used to do on the normal Warehouse normal table okay so you don't know about scenes like there is some confusion you might be talking about the databricks you started talking about the database not only the ADF because you cannot do this the email operation Etc in the the tables which are going to create like a spark tables okay okay okay okay okay okay you getting my appointment I'll come back to that in a few minutes okay so that all this get done okay now assume that all this is working fine so I assume you're using the database as well because as you said you're using for cleaning and all what size of data are getting in we are getting a play 5gb of data pipeline which is your talking about or overall you're talking about super pipeline we are getting a nearly half a GB of data per pipeline ok okay and you are doing that cleaning in the database using the spark okay how would you do this park performance optimization okay so there are an animal of using a spark performance the first way is that we can like if there are two tables on with the suppose joining is happening and we will see like if suppose any one of the table is smaller one we can go for broadcasting station so that during the joining operation essentially shuffling will be awarded because it will go and like each other table can join with a smaller table without any of so what we say data passing across what's the partition can we stopped and our time can be reduced this is one another one is we can use a broadcast joint sorry broadcast then we can use that this bucket bucket joint and this is the data is stored in form of like we can say on the lake joining is happening if on basis of country country so that whatever table which are there we will put them in Partition by Group by by country and the other table is also that and if the joining key for in case country they were also fall back in different different containers on Facebook of the country I just there is one condition that number of buckets for table one and table two should be same and then only can happen other than that there is something called as a repartition this we are doing to reduce the data skewness and apart from this there is an Adaptive which is Park internal optimization that we don't need to take care of spark on its own can optimize the query performance okay great thank you so much so I think let's get into the second half of this thank you for coming in so in the second half of this will analyze and will understand like where you did good and where you did little bit Improvement is needed okay so let's get started with this assessment so it's starting with it your presentation is good your comes is good so there you are doing perfectly well second thing is at 1:07 like we are pulling the data from data services which is one of them is ODC so it's not operational data store okay second thing I feel like you wanted to talk about the sequels are but you end up with talking as you sequel database so you cannot do Christmas Day when you saying that my source is on on Prem and then I'm pulling the data from on Prem when somebody ask you what is your on-prem TV answer is crossword clue sequel server maybe in our case you might want to say sequel server but you end up with saying that as your sequel DB which is not right because as your secrets or I just sequel server is basically Cloud one it's not a on Prem okay so keep I mean you know take care of that then your project explanation startup is good perfectly fine that is okay so that is you are ready with it then then I can probably have asked is about the challenging situation which you come up with the right only one thing which I would love would be is like you don't say how you did it until unless I asked so I asked so I want you to go on that that you said whatever was the problem you take a nap you set the stage video rightly then you leave it ideally rather than saying it like how you solve that let's wait for that question to come in okay then how you solve it so you just explain the problem and pause okay and let the interviewer ask okay then the third thing which we talked about is how you do the on-prem to the cloud right then you talked about okay we use a copy activity and then when I asked for and I ask it like only the top activity then you said about that we need to have you got a little bit fumble about about the integration run time okay so and then you talk about that we need a cell phone indication on time instead of that the auto integration and type but you are not very clear why we need the cell phone so the answer is like Auto integration on time is picking and calling it from random IP addresses so probably your private Network cannot get reached from add random IP addresses right because that is not something which is bind okay so probably that is probably your your private network will not allow to that connection to get Pastor because of your firewall and all and that's where we need a selfish indication on time so that I will be a fixed one which is connected to your database server because your database server is expecting a call from a limited IP addresses only and then yourself first and machine will get fixed and then that's IP address will be you know whitelisted by the firewall of your database server so that that connection get passed through so that is something which you need to work on you remember we have covered in our sessions okay so just go back to those questions and you know cover that the why we need the cell phone here instead of the your on Prem sorry for your auto resolve okay then we talked about your project okay then okay dad comes in and then how you going to you know how are you going to take the data forward after your lending data comes in probably at that piece ideally yes for the process maybe it's more of like either of you are copying into the bronze Lair via data breaks if you remember okay the project is not talking about is as you said as you mentioned it that it is something that you were talking about in the database side so your data is getting saved into the Browns player as a daily table but you creating those data table using the database right you create the bronze layer database and then you create your storing that translate tables as a brown Stables as a Delta table and under the hood table will get save as a parking lot you can go and do all that email operation generally normal operation is not allowed on normal spark tables but when you make a table as a Delta table then you can apply all that so then the floor is like from there once you get the data from your on-premise is there an 80s you go back to your databricks and from database you are fetching that you know tables and pushing it into using the mounting and all so I think that piece you got missed okay so just cover that up then when we talked about about this size of the data for your pipeline you said it like 5 GB initially then you get to the 0.5 GB you know so that is something which you know that's an expected question which you can always you know have a clear cut in the mind that what is the correct size okay so don't get confused between the 0.55 it was okay but why do you look very small so when you give me a very small thing you have to kiss Krystal pipeline per day something like that you getting my point so you have to Crystal Clear Define it so that eventually your number should look bigger the whole idea is that number should look bigger so rather than talking about less number try to talk about a bigger number okay and then from that you cut down to the lower number rather than take out on Lower number so you might start thinking about that one is my having a 100gb or like 100 times a pipeline in a month something like that over this is that the last week we talked about the spark optimization so for the spark optimization yeah I mean I understand you trying to talk about all those Concepts but I feel that it could be further more you know you should create a kind of a scenario and then talk about it because otherwise we will be very generous so maybe when I asked that okay you got to find it terabytes of 1 tablespoon or something like that then how you will optimize your this sparkle then you must come up with something like that okay so let's assume that on this a large database on largest generally whenever we have a small I mean when we have an arrow transformation problem does not happen so mostly this information you need to come in when we get into the kind of a white transformation so I assume that we have a lot of white transformation because of that probably this problem is coming up got it so that that's how you start up with and then you talk about it okay that will be a joint is there so now we might do a joint optimization so there could be a multiple ways in which we can do a joint optimization maybe we could do kind of repartition do using a bucket thing using something like software join or some different techniques right so that's how you slowly slowly progress it and then another good thing is that you are talking about the Delta tables okay so then there is a performance Automation in the Delta Lake of its own like you have an optimized Command right or you can run a vacuum command also which can also you know cut down some extra data which is not automatically you know delete it which is outside the retention period so all those things you keep in mind but I mean to to make your story looks more brighter if somebody is asking a very big question something like that tell me how you can optimize it so you can create a scenario around it okay so that your answer could looks more Justified okay so for example talk about the country now country is out of the scope right I mean so then you have to find your data set and then talk about the country okay so let's say for example and now let's say we are doing a joining around it so maybe we can partition the database on the country and then it will make sense to optimize it you're getting my point so that's how it's not that I mean bad but I feel like how you can make it more you know looks more better so that could be few points otherwise I will only trying too much digging into the project because a lot of people was wondering to check it like how the project think goes on so probably these points I think in the real interview maybe you not get too much dick down onto this project yeah I was purposely doing it so I hope you get you get the point like where you have to work on so yes you can connect another time after you prepare for all this and then we have a one more round of interview okay great I hope you enjoyed it and those who are watching this also enjoying and learn a lot thank you so much for coming on this see you soon love you

